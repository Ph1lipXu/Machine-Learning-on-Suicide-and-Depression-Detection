{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ph1lipXu/Machine-Learning-on-Suicide-and-Depression-Detection/blob/main/Modeling_16000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jx_vlHyX_UCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow nltk scikit-learn\n",
        "#!pip install gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, SimpleRNN, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "pd.options.display.max_columns = 20\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.max_colwidth = 80\n",
        "np.set_printoptions(precision = 4, suppress = True)"
      ],
      "metadata": {
        "id": "wZAGxndo_YKM",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrqfA4dE_QDL",
        "outputId": "8277fc0a-5deb-48a6-d3fd-6a6d97d4aae7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-10 19:50:01--  https://raw.githubusercontent.com/Ph1lipXu/Machine-Learning-on-Suicide-and-Depression-Detection/refs/heads/main/data/cleaned_data_16000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36085156 (34M) [text/plain]\n",
            "Saving to: ‘cleaned_data_16000.csv’\n",
            "\n",
            "cleaned_data_16000. 100%[===================>]  34.41M   112MB/s    in 0.3s    \n",
            "\n",
            "2025-04-10 19:50:03 (112 MB/s) - ‘cleaned_data_16000.csv’ saved [36085156/36085156]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Ph1lipXu/Machine-Learning-on-Suicide-and-Depression-Detection/refs/heads/main/data/cleaned_data_16000.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/cleaned_data_16000.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IWg9ccPYY68k",
        "outputId": "c836af2c-392a-4682-ee1b-76c8c7ff18fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                              text  \\\n",
              "0  can i get some support please...so i am not as depressed as i used to be (i ...   \n",
              "1  everything is going wrong .i have been trying not to drink, but everyone is ...   \n",
              "2                                                     i am done fighting it.*gone*   \n",
              "3  today i cut my hairmy hair has always been a thick mess of curls that went a...   \n",
              "4  i do not know what to do and i have no hopes for the future.it is kinda toug...   \n",
              "\n",
              "        class  \\\n",
              "0  depression   \n",
              "1  depression   \n",
              "2  depression   \n",
              "3  depression   \n",
              "4  depression   \n",
              "\n",
              "                                                                            tokens  \n",
              "0  ['can', 'I', 'get', 'some', 'support', 'please', 'so', 'I', 'be', 'not', 'as...  \n",
              "1  ['everything', 'be', 'go', 'wrong', 'have', 'be', 'try', 'not', 'to', 'drink...  \n",
              "2                                           ['I', 'be', 'do', 'fight', 'it', 'go']  \n",
              "3  ['today', 'I', 'cut', 'my', 'hairmy', 'hair', 'have', 'always', 'be', 'a', '...  \n",
              "4  ['I', 'do', 'not', 'know', 'what', 'to', 'do', 'and', 'I', 'have', 'no', 'ho...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a36e0c58-bb9c-4ab5-a351-58c7d0788630\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>can i get some support please...so i am not as depressed as i used to be (i ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['can', 'I', 'get', 'some', 'support', 'please', 'so', 'I', 'be', 'not', 'as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>everything is going wrong .i have been trying not to drink, but everyone is ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['everything', 'be', 'go', 'wrong', 'have', 'be', 'try', 'not', 'to', 'drink...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am done fighting it.*gone*</td>\n",
              "      <td>depression</td>\n",
              "      <td>['I', 'be', 'do', 'fight', 'it', 'go']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>today i cut my hairmy hair has always been a thick mess of curls that went a...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['today', 'I', 'cut', 'my', 'hairmy', 'hair', 'have', 'always', 'be', 'a', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i do not know what to do and i have no hopes for the future.it is kinda toug...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['I', 'do', 'not', 'know', 'what', 'to', 'do', 'and', 'I', 'have', 'no', 'ho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a36e0c58-bb9c-4ab5-a351-58c7d0788630')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a36e0c58-bb9c-4ab5-a351-58c7d0788630 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a36e0c58-bb9c-4ab5-a351-58c7d0788630');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed2b18e7-9b09-4061-b477-35ef7c47ebbe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed2b18e7-9b09-4061-b477-35ef7c47ebbe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed2b18e7-9b09-4061-b477-35ef7c47ebbe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12918,\n        \"samples\": [\n          \"i cannot go through with iti have made the decision 4 ti ames, yet every ti ame i seem to back out last minute. i was so sure i would go through with it this ti ame, too. i quit my job, broke up with boyfriend, deleted most of my social media accounts, i was ready to just end it all. yet for some reason i still cannot work up the courage. currently parked off the road next to a bridge because i am scared and do not know what to do.\",\n          \"anyone wanna get married ? bc you can only have sex after marriage  flushed_face \",\n          \"this saturdaythat would be a good day for it. ive finally escaped peoples birthdays, so i neednt worry about that. i just need to get things settled by then.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"SuicideWatch\",\n          \"noise\",\n          \"depression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12908,\n        \"samples\": [\n          \"['at', 'what', 'ti', 'ame', 'do', 'you', 'usually', 'workout', 'personally', 'I', 'just', 'try', 'to', 'be', 'randomly', 'active', 'throughout', 'the', 'day', 'cause', 'I', 'do', 'not', 'know', 'how', 'to', 'focus', 'myself', 'into', 'a', 'workout', 'most', 'of', 'the', 'ti', 'ame', 'which', 'be', 'really', 'I', 'amportant', 'to', 'I', 'because', 'constantly', 'force', 'yourself', 'to', 'do', 'something', 'take', 'a', 'huge', 'toll', 'on', 'your', 'mental', 'health', 'I', 'have', 'try', 'to', 'get', 'into', 'moright', 'nowe', 'workout', 'but', 'I', 'be', 'never', 'feel', 'it', 'or', 'I', 'sleep', 'in', 'usually', 'I', 'start', 'around', 'so', 'I', 'can', 'sleep', 'like', 'the', 'dead', 'even', 'thoughugh', 'they', 'say', 'you', 'should', 'not', 'do', 'that', 'but', 'I', 'be', 'type', 'this', 'at', 'am', 'because', 'I', 'mess', 'up', 'my', 'sleep', 'schedule', 'in', 'the', 'past', 'week', 'I', 'be', 'wonder', 'if', 'I', 'should', 'use', 'this', 'opportunity', 'to', 'try', 'and', 'start', 'a', 'moright', 'nowing', 'routine', 'again']\",\n          \"['anyone', 'wan', 'na', 'make', 'a', 'discord', 'server', 'together', 'it', 'can', 'be', 'about', 'whatever', 'you', 'want', 'I', 'be', 'just', 'that', 'bored', 'really']\",\n          \"['I', 'make', 'this', 'survey', 'on', 'social', 'comparison', 'and', 'in', 'psychology', 'hey', 'I', 'be', 'do', 'an', 'anonymous', 'survey', 'about', 'social', 'comparison', 'and', 'as', 'a', 'task', 'in', 'psychology', 'class', 'I', 'would', 'greatly', 'appreciate', 'if', 'anybody', 'would', 'like', 'to', 'participate', 'thank', 'in', 'advance', 'link', 'https']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHw9aZRJ7aBI"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['suicide_class'] = df['class'].apply(lambda x: 'suicide' if x == 'SuicideWatch' else 'nonsuicide')\n",
        "df['depression_class'] = df['class'].apply(lambda x: 'depression' if x == 'depression' else 'nondepression')\n",
        "df['teenager_class'] = df['class'].apply(lambda x: 'teenager' if x == 'teenagers' else 'nonteenager')"
      ],
      "metadata": {
        "id": "RvGbnNesc6-N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "mnf7KB_cc8Mu",
        "outputId": "7f7e076d-2081-492f-9b61-ccdda6054281"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                              text  \\\n",
              "0  can i get some support please...so i am not as depressed as i used to be (i ...   \n",
              "1  everything is going wrong .i have been trying not to drink, but everyone is ...   \n",
              "2                                                     i am done fighting it.*gone*   \n",
              "3  today i cut my hairmy hair has always been a thick mess of curls that went a...   \n",
              "4  i do not know what to do and i have no hopes for the future.it is kinda toug...   \n",
              "5  tired of life, tired of living. do not know what to do hey guys, \\n\\ni am 16...   \n",
              "6  what is one concrete thing that has helped you in your battle against depres...   \n",
              "7  does mental health go hand in hand with the physical health?when i feel at m...   \n",
              "8  the thing that hurts the most is knowing that i have been through worse.when...   \n",
              "9  need someone to talk toi am a guy in high school and i just need to talk to ...   \n",
              "\n",
              "        class  \\\n",
              "0  depression   \n",
              "1  depression   \n",
              "2  depression   \n",
              "3  depression   \n",
              "4  depression   \n",
              "5  depression   \n",
              "6  depression   \n",
              "7  depression   \n",
              "8  depression   \n",
              "9  depression   \n",
              "\n",
              "                                                                            tokens  \\\n",
              "0  ['can', 'I', 'get', 'some', 'support', 'please', 'so', 'I', 'be', 'not', 'as...   \n",
              "1  ['everything', 'be', 'go', 'wrong', 'have', 'be', 'try', 'not', 'to', 'drink...   \n",
              "2                                           ['I', 'be', 'do', 'fight', 'it', 'go']   \n",
              "3  ['today', 'I', 'cut', 'my', 'hairmy', 'hair', 'have', 'always', 'be', 'a', '...   \n",
              "4  ['I', 'do', 'not', 'know', 'what', 'to', 'do', 'and', 'I', 'have', 'no', 'ho...   \n",
              "5  ['tired', 'of', 'life', 'tired', 'of', 'live', 'do', 'not', 'know', 'what', ...   \n",
              "6  ['what', 'be', 'one', 'concrete', 'thing', 'that', 'have', 'help', 'you', 'i...   \n",
              "7  ['do', 'mental', 'health', 'go', 'hand', 'in', 'hand', 'with', 'the', 'physi...   \n",
              "8  ['the', 'thing', 'that', 'hurt', 'the', 'most', 'be', 'know', 'that', 'I', '...   \n",
              "9  ['need', 'someone', 'to', 'talk', 'toi', 'be', 'a', 'guy', 'in', 'high', 'sc...   \n",
              "\n",
              "  suicide_class depression_class teenager_class  \n",
              "0    nonsuicide       depression    nonteenager  \n",
              "1    nonsuicide       depression    nonteenager  \n",
              "2    nonsuicide       depression    nonteenager  \n",
              "3    nonsuicide       depression    nonteenager  \n",
              "4    nonsuicide       depression    nonteenager  \n",
              "5    nonsuicide       depression    nonteenager  \n",
              "6    nonsuicide       depression    nonteenager  \n",
              "7    nonsuicide       depression    nonteenager  \n",
              "8    nonsuicide       depression    nonteenager  \n",
              "9    nonsuicide       depression    nonteenager  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-646e255f-5ea6-4e67-bbde-9ef76ce1d3fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>tokens</th>\n",
              "      <th>suicide_class</th>\n",
              "      <th>depression_class</th>\n",
              "      <th>teenager_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>can i get some support please...so i am not as depressed as i used to be (i ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['can', 'I', 'get', 'some', 'support', 'please', 'so', 'I', 'be', 'not', 'as...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>everything is going wrong .i have been trying not to drink, but everyone is ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['everything', 'be', 'go', 'wrong', 'have', 'be', 'try', 'not', 'to', 'drink...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am done fighting it.*gone*</td>\n",
              "      <td>depression</td>\n",
              "      <td>['I', 'be', 'do', 'fight', 'it', 'go']</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>today i cut my hairmy hair has always been a thick mess of curls that went a...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['today', 'I', 'cut', 'my', 'hairmy', 'hair', 'have', 'always', 'be', 'a', '...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i do not know what to do and i have no hopes for the future.it is kinda toug...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['I', 'do', 'not', 'know', 'what', 'to', 'do', 'and', 'I', 'have', 'no', 'ho...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tired of life, tired of living. do not know what to do hey guys, \\n\\ni am 16...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['tired', 'of', 'life', 'tired', 'of', 'live', 'do', 'not', 'know', 'what', ...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>what is one concrete thing that has helped you in your battle against depres...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['what', 'be', 'one', 'concrete', 'thing', 'that', 'have', 'help', 'you', 'i...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>does mental health go hand in hand with the physical health?when i feel at m...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['do', 'mental', 'health', 'go', 'hand', 'in', 'hand', 'with', 'the', 'physi...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>the thing that hurts the most is knowing that i have been through worse.when...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['the', 'thing', 'that', 'hurt', 'the', 'most', 'be', 'know', 'that', 'I', '...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>need someone to talk toi am a guy in high school and i just need to talk to ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>['need', 'someone', 'to', 'talk', 'toi', 'be', 'a', 'guy', 'in', 'high', 'sc...</td>\n",
              "      <td>nonsuicide</td>\n",
              "      <td>depression</td>\n",
              "      <td>nonteenager</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-646e255f-5ea6-4e67-bbde-9ef76ce1d3fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-646e255f-5ea6-4e67-bbde-9ef76ce1d3fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-646e255f-5ea6-4e67-bbde-9ef76ce1d3fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c21ede7e-8f4f-4101-8c97-db0afe6b2398\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c21ede7e-8f4f-4101-8c97-db0afe6b2398')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c21ede7e-8f4f-4101-8c97-db0afe6b2398 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12918,\n        \"samples\": [\n          \"i cannot go through with iti have made the decision 4 ti ames, yet every ti ame i seem to back out last minute. i was so sure i would go through with it this ti ame, too. i quit my job, broke up with boyfriend, deleted most of my social media accounts, i was ready to just end it all. yet for some reason i still cannot work up the courage. currently parked off the road next to a bridge because i am scared and do not know what to do.\",\n          \"anyone wanna get married ? bc you can only have sex after marriage  flushed_face \",\n          \"this saturdaythat would be a good day for it. ive finally escaped peoples birthdays, so i neednt worry about that. i just need to get things settled by then.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"SuicideWatch\",\n          \"noise\",\n          \"depression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12908,\n        \"samples\": [\n          \"['at', 'what', 'ti', 'ame', 'do', 'you', 'usually', 'workout', 'personally', 'I', 'just', 'try', 'to', 'be', 'randomly', 'active', 'throughout', 'the', 'day', 'cause', 'I', 'do', 'not', 'know', 'how', 'to', 'focus', 'myself', 'into', 'a', 'workout', 'most', 'of', 'the', 'ti', 'ame', 'which', 'be', 'really', 'I', 'amportant', 'to', 'I', 'because', 'constantly', 'force', 'yourself', 'to', 'do', 'something', 'take', 'a', 'huge', 'toll', 'on', 'your', 'mental', 'health', 'I', 'have', 'try', 'to', 'get', 'into', 'moright', 'nowe', 'workout', 'but', 'I', 'be', 'never', 'feel', 'it', 'or', 'I', 'sleep', 'in', 'usually', 'I', 'start', 'around', 'so', 'I', 'can', 'sleep', 'like', 'the', 'dead', 'even', 'thoughugh', 'they', 'say', 'you', 'should', 'not', 'do', 'that', 'but', 'I', 'be', 'type', 'this', 'at', 'am', 'because', 'I', 'mess', 'up', 'my', 'sleep', 'schedule', 'in', 'the', 'past', 'week', 'I', 'be', 'wonder', 'if', 'I', 'should', 'use', 'this', 'opportunity', 'to', 'try', 'and', 'start', 'a', 'moright', 'nowing', 'routine', 'again']\",\n          \"['anyone', 'wan', 'na', 'make', 'a', 'discord', 'server', 'together', 'it', 'can', 'be', 'about', 'whatever', 'you', 'want', 'I', 'be', 'just', 'that', 'bored', 'really']\",\n          \"['I', 'make', 'this', 'survey', 'on', 'social', 'comparison', 'and', 'in', 'psychology', 'hey', 'I', 'be', 'do', 'an', 'anonymous', 'survey', 'about', 'social', 'comparison', 'and', 'as', 'a', 'task', 'in', 'psychology', 'class', 'I', 'would', 'greatly', 'appreciate', 'if', 'anybody', 'would', 'like', 'to', 'participate', 'thank', 'in', 'advance', 'link', 'https']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"suicide_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"suicide\",\n          \"nonsuicide\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"depression_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"nondepression\",\n          \"depression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"teenager_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"teenager\",\n          \"nonteenager\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKZo9e-pR9UD"
      },
      "source": [
        "## Suicide / Non-Suicide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNebm64XTRpo"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "processed_df[\"label\"] = label_encoder.fit_transform(processed_df[\"suicide_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs-OLe6209Yo",
        "outputId": "a12b9f0d-9a1f-4692-ad05-fef0fc56bfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data:  12000\n",
            "Testing data:  4000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Instead of random train test split, stratify by class\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    processed_df[\"tokens\"], processed_df[\"label\"], random_state=64, stratify=processed_df['label']\n",
        ")\n",
        "print('Training data: ',len(train_texts))\n",
        "print('Testing data: ',len(test_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCo8KR7TsLpe"
      },
      "outputs": [],
      "source": [
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQe7527viLtq",
        "outputId": "73c0d251-7a66-4f77-b32a-808f7c97afec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1435     [anyone, else, feel, like, this, I, have, be, cope, with, depression, for, a...\n",
            "1368     [lose, my, sense, of, realityit, be, another, one, of, thoughse, night, and,...\n",
            "5863                                                    [good, way, to, commit, suicide]\n",
            "8929     [I, be, bakk, you, lousy, son, of, bitch, thoughught, I, be, do, I, give, yo...\n",
            "15448                                                                [no, text, content]\n",
            "11663    [just, realize, how, close, my, teenage, year, be, to, finish, I, be, and, I...\n",
            "3897     [just, a, thoughught, that, enter, my, mind, lie, in, bed, at, not, able, to...\n",
            "4635     [why, when, I, die, the, world, will, not, stop, spin, nothing, change, ti, ...\n",
            "14158                                                                [no, text, content]\n",
            "13454                                                                [no, text, content]\n",
            "Name: tokens, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(train_texts.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhtvxmjmBQT"
      },
      "source": [
        "### Vectorization/Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0da8ceRTxxe"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding sequences to have the same length\n",
        "max_len = 200  # Max length for padding\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi3JxsDHTvF8"
      },
      "outputs": [],
      "source": [
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_data, val_labels = self.validation_data\n",
        "        val_preds = self.model.predict(val_data)\n",
        "        val_preds = np.argmax(val_preds, axis=1)  # Convert probabilities to class labels\n",
        "        val_labels = np.argmax(val_labels, axis=1)  # Convert one-hot labels to class labels\n",
        "\n",
        "        f1 = f1_score(val_labels, val_preds, average='weighted')  # Change to 'macro' if needed\n",
        "        print(f\" - val_f1: {f1:.4f}\")\n",
        "        logs[\"val_f1\"] = f1  # Store it in logs if needed"
      ],
      "metadata": {
        "id": "HXNQxOpT3kLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cjJ-leXT_Ci"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(vocab_size, embedding_matrix, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_rnn_model(vocab_size, embedding_matrix, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_bilstm_model(vocab_size, embedding_matrix, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "        Bidirectional(LSTM(128)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate(model, train_padded, train_labels, test_padded, test_labels, epochs=10, batch_size=32):\n",
        "    f1_callback = F1ScoreCallback(validation_data=(test_padded, test_labels))\n",
        "\n",
        "    model.fit(train_padded, train_labels,\n",
        "              epochs=epochs, batch_size=batch_size,\n",
        "              validation_data=(test_padded, test_labels),\n",
        "              callbacks=[f1_callback])\n",
        "\n",
        "    loss, acc = model.evaluate(test_padded, test_labels)\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2P6arsDUH9X",
        "outputId": "a794119a-e527-4ad4-fd2c-4eb6470082f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.8046\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 660ms/step - accuracy: 0.7535 - loss: 0.4779 - val_accuracy: 0.8127 - val_loss: 0.3835 - val_f1: 0.8046\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 182ms/step\n",
            " - val_f1: 0.8145\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 660ms/step - accuracy: 0.8110 - loss: 0.3813 - val_accuracy: 0.8242 - val_loss: 0.3606 - val_f1: 0.8145\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step\n",
            " - val_f1: 0.7994\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 726ms/step - accuracy: 0.8303 - loss: 0.3551 - val_accuracy: 0.8150 - val_loss: 0.4182 - val_f1: 0.7994\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.8228\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 623ms/step - accuracy: 0.8340 - loss: 0.3535 - val_accuracy: 0.8330 - val_loss: 0.3648 - val_f1: 0.8228\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step\n",
            " - val_f1: 0.8388\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 611ms/step - accuracy: 0.8643 - loss: 0.3010 - val_accuracy: 0.8440 - val_loss: 0.3561 - val_f1: 0.8388\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 171ms/step\n",
            " - val_f1: 0.8323\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 664ms/step - accuracy: 0.8848 - loss: 0.2675 - val_accuracy: 0.8335 - val_loss: 0.3732 - val_f1: 0.8323\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step\n",
            " - val_f1: 0.8308\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 670ms/step - accuracy: 0.9035 - loss: 0.2279 - val_accuracy: 0.8342 - val_loss: 0.3882 - val_f1: 0.8308\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 182ms/step\n",
            " - val_f1: 0.8335\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 689ms/step - accuracy: 0.9180 - loss: 0.1912 - val_accuracy: 0.8370 - val_loss: 0.4269 - val_f1: 0.8335\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 187ms/step\n",
            " - val_f1: 0.8216\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 728ms/step - accuracy: 0.9360 - loss: 0.1552 - val_accuracy: 0.8250 - val_loss: 0.5506 - val_f1: 0.8216\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 188ms/step\n",
            " - val_f1: 0.8225\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 649ms/step - accuracy: 0.9504 - loss: 0.1188 - val_accuracy: 0.8190 - val_loss: 0.5434 - val_f1: 0.8225\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 178ms/step - accuracy: 0.8233 - loss: 0.5395\n",
            "Test Accuracy: 0.8190\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jS-KZPWW4rS",
        "outputId": "24e23803-fa62-49bc-8f1a-a766ea321865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 185ms/step\n",
            " - val_f1: 0.7444\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 681ms/step - accuracy: 0.7477 - loss: 0.4818 - val_accuracy: 0.7845 - val_loss: 0.3952 - val_f1: 0.7444\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 186ms/step\n",
            " - val_f1: 0.8111\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 722ms/step - accuracy: 0.7967 - loss: 0.4043 - val_accuracy: 0.8045 - val_loss: 0.3845 - val_f1: 0.8111\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 191ms/step\n",
            " - val_f1: 0.7504\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 729ms/step - accuracy: 0.8294 - loss: 0.3570 - val_accuracy: 0.7383 - val_loss: 0.4499 - val_f1: 0.7504\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 186ms/step\n",
            " - val_f1: 0.8132\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 678ms/step - accuracy: 0.8182 - loss: 0.3601 - val_accuracy: 0.8090 - val_loss: 0.3803 - val_f1: 0.8132\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.8259\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 706ms/step - accuracy: 0.8643 - loss: 0.2955 - val_accuracy: 0.8278 - val_loss: 0.3757 - val_f1: 0.8259\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step\n",
            " - val_f1: 0.8121\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 722ms/step - accuracy: 0.8787 - loss: 0.2717 - val_accuracy: 0.8085 - val_loss: 0.3897 - val_f1: 0.8121\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 188ms/step\n",
            " - val_f1: 0.8237\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 677ms/step - accuracy: 0.8928 - loss: 0.2489 - val_accuracy: 0.8250 - val_loss: 0.4141 - val_f1: 0.8237\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 187ms/step\n",
            " - val_f1: 0.8239\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 724ms/step - accuracy: 0.9158 - loss: 0.2082 - val_accuracy: 0.8210 - val_loss: 0.4280 - val_f1: 0.8239\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 177ms/step\n",
            " - val_f1: 0.8170\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 677ms/step - accuracy: 0.9226 - loss: 0.1802 - val_accuracy: 0.8158 - val_loss: 0.5028 - val_f1: 0.8170\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step\n",
            " - val_f1: 0.8142\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 677ms/step - accuracy: 0.9455 - loss: 0.1337 - val_accuracy: 0.8185 - val_loss: 0.5799 - val_f1: 0.8142\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.8303 - loss: 0.5548\n",
            "Test Accuracy: 0.8185\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Bi-LSTM with FastText Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi8P_Yw_UdH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b2ed7a-5bfa-47a0-e725-86a6cbd87657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.7749\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - accuracy: 0.7245 - loss: 0.6417 - val_accuracy: 0.8025 - val_loss: 0.3911 - val_f1: 0.7749\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step\n",
            " - val_f1: 0.8299\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.8186 - loss: 0.3763 - val_accuracy: 0.8307 - val_loss: 0.3693 - val_f1: 0.8299\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8365\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - accuracy: 0.8435 - loss: 0.3392 - val_accuracy: 0.8388 - val_loss: 0.3523 - val_f1: 0.8365\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.8305\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 60ms/step - accuracy: 0.8601 - loss: 0.3041 - val_accuracy: 0.8245 - val_loss: 0.3658 - val_f1: 0.8305\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
            " - val_f1: 0.8359\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - accuracy: 0.8777 - loss: 0.2698 - val_accuracy: 0.8317 - val_loss: 0.3722 - val_f1: 0.8359\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8323\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 61ms/step - accuracy: 0.8951 - loss: 0.2372 - val_accuracy: 0.8322 - val_loss: 0.3693 - val_f1: 0.8323\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8261\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.9165 - loss: 0.2019 - val_accuracy: 0.8328 - val_loss: 0.4796 - val_f1: 0.8261\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8300\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.9329 - loss: 0.1618 - val_accuracy: 0.8288 - val_loss: 0.4699 - val_f1: 0.8300\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step\n",
            " - val_f1: 0.7933\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9493 - loss: 0.1335 - val_accuracy: 0.8138 - val_loss: 0.6508 - val_f1: 0.7933\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.8175\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9573 - loss: 0.1136 - val_accuracy: 0.8188 - val_loss: 0.6042 - val_f1: 0.8175\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8206 - loss: 0.6084\n",
            "Test Accuracy: 0.8188\n"
          ]
        }
      ],
      "source": [
        "print(\"Training CNN with Word2Vec Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdp_PZ8vXCOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ecaa9c3-33da-41ee-bead-3a366a618620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6463\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - accuracy: 0.7104 - loss: 0.6730 - val_accuracy: 0.7513 - val_loss: 0.4629 - val_f1: 0.6463\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7725\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.7590 - loss: 0.4579 - val_accuracy: 0.7990 - val_loss: 0.4166 - val_f1: 0.7725\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step\n",
            " - val_f1: 0.7673\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.7898 - loss: 0.4229 - val_accuracy: 0.8000 - val_loss: 0.4021 - val_f1: 0.7673\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8186\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - accuracy: 0.8060 - loss: 0.4014 - val_accuracy: 0.8238 - val_loss: 0.3831 - val_f1: 0.8186\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8108\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - accuracy: 0.8185 - loss: 0.3749 - val_accuracy: 0.8213 - val_loss: 0.3869 - val_f1: 0.8108\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7952\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.8296 - loss: 0.3598 - val_accuracy: 0.8155 - val_loss: 0.3997 - val_f1: 0.7952\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8214\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.8240 - loss: 0.3509 - val_accuracy: 0.8235 - val_loss: 0.3770 - val_f1: 0.8214\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
            " - val_f1: 0.8204\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - accuracy: 0.8437 - loss: 0.3302 - val_accuracy: 0.8217 - val_loss: 0.3829 - val_f1: 0.8204\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step\n",
            " - val_f1: 0.8091\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 59ms/step - accuracy: 0.8446 - loss: 0.3248 - val_accuracy: 0.8167 - val_loss: 0.3932 - val_f1: 0.8091\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8164\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.8560 - loss: 0.2999 - val_accuracy: 0.8098 - val_loss: 0.4074 - val_f1: 0.8164\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8096 - loss: 0.4107\n",
            "Test Accuracy: 0.8098\n"
          ]
        }
      ],
      "source": [
        "print(\"Training CNN with FastText Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf3hFIFYAOZ"
      },
      "source": [
        "### Tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHSomU-CYBiO"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "processed_df[\"label\"] = label_encoder.fit_transform(processed_df[\"suicide_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvaumW1YYFn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10195673-6a59-4018-9597-08e0413079fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step\n",
            " - val_f1: 0.7907\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 302ms/step - accuracy: 0.7560 - loss: 0.4691 - val_accuracy: 0.8072 - val_loss: 0.3875 - val_f1: 0.7907\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step\n",
            " - val_f1: 0.8307\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 297ms/step - accuracy: 0.8197 - loss: 0.3654 - val_accuracy: 0.8328 - val_loss: 0.3527 - val_f1: 0.8307\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step\n",
            " - val_f1: 0.8270\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 297ms/step - accuracy: 0.8354 - loss: 0.3416 - val_accuracy: 0.8309 - val_loss: 0.3565 - val_f1: 0.8270\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step\n",
            " - val_f1: 0.7921\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 356ms/step - accuracy: 0.8559 - loss: 0.3130 - val_accuracy: 0.8119 - val_loss: 0.3930 - val_f1: 0.7921\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step\n",
            " - val_f1: 0.8242\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 304ms/step - accuracy: 0.8733 - loss: 0.2758 - val_accuracy: 0.8288 - val_loss: 0.3828 - val_f1: 0.8242\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 82ms/step\n",
            " - val_f1: 0.8262\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 304ms/step - accuracy: 0.9008 - loss: 0.2303 - val_accuracy: 0.8319 - val_loss: 0.4316 - val_f1: 0.8262\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step\n",
            " - val_f1: 0.8108\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 303ms/step - accuracy: 0.9214 - loss: 0.1883 - val_accuracy: 0.8163 - val_loss: 0.4668 - val_f1: 0.8108\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step\n",
            " - val_f1: 0.8165\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 302ms/step - accuracy: 0.9391 - loss: 0.1418 - val_accuracy: 0.8213 - val_loss: 0.5361 - val_f1: 0.8165\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step\n",
            " - val_f1: 0.8158\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 299ms/step - accuracy: 0.9607 - loss: 0.0983 - val_accuracy: 0.8159 - val_loss: 0.6348 - val_f1: 0.8158\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step\n",
            " - val_f1: 0.8222\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 296ms/step - accuracy: 0.9731 - loss: 0.0742 - val_accuracy: 0.8219 - val_loss: 0.7226 - val_f1: 0.8222\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.8110 - loss: 0.7782\n",
            "Test Accuracy: 0.8219\n"
          ]
        }
      ],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(processed_df[\"tokens\"], processed_df[\"label\"], test_size=0.2, random_state=64)\n",
        "\n",
        "# Train Word2Vec and FastText models\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts.apply(' '.join))  # Join tokens back to text for the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Create embedding matrices\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]\n",
        "\n",
        "max_len = 100  # Max length for padding\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts.apply(' '.join))  # Join tokens for sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts.apply(' '.join))\n",
        "\n",
        "# Padding sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "# Train Bi-LSTM with Word2Vec embeddings\n",
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depression / Non-Depression"
      ],
      "metadata": {
        "id": "0ZYEKPCQfRSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depression_df['depression_class'] = depression_df['class'].apply(lambda x: 'depression' if x == 'depression' else 'nondepression')"
      ],
      "metadata": {
        "id": "T5DD-VDDfbnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "depression_df[\"label\"] = label_encoder.fit_transform(depression_df[\"depression_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "5w1LbU5wf_5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Instead of random train test split, stratify by class\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    depression_df[\"tokens\"], depression_df[\"label\"], random_state=64, stratify=depression_df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "Xj4PsEbphVsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "JqB-0wFEhY8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization/Embedding"
      ],
      "metadata": {
        "id": "MZ9L_UFphfMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding sequences to have the same length\n",
        "max_len = 200  # Max length for padding\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "dVjANGhBhiyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]"
      ],
      "metadata": {
        "id": "P8Ia4s8thpKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "gVe6qK6sik3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b025cc17-099e-4800-df60-cc733b4f7461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 182ms/step\n",
            " - val_f1: 0.7729\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 611ms/step - accuracy: 0.7374 - loss: 0.4883 - val_accuracy: 0.7828 - val_loss: 0.4120 - val_f1: 0.7729\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 183ms/step\n",
            " - val_f1: 0.7709\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 705ms/step - accuracy: 0.8076 - loss: 0.3888 - val_accuracy: 0.7815 - val_loss: 0.4130 - val_f1: 0.7709\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step\n",
            " - val_f1: 0.8040\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 712ms/step - accuracy: 0.8210 - loss: 0.3690 - val_accuracy: 0.8008 - val_loss: 0.4021 - val_f1: 0.8040\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 179ms/step\n",
            " - val_f1: 0.7947\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 623ms/step - accuracy: 0.8395 - loss: 0.3433 - val_accuracy: 0.8130 - val_loss: 0.3881 - val_f1: 0.7947\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 183ms/step\n",
            " - val_f1: 0.8212\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 605ms/step - accuracy: 0.8505 - loss: 0.3219 - val_accuracy: 0.8202 - val_loss: 0.3846 - val_f1: 0.8212\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 188ms/step\n",
            " - val_f1: 0.8177\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 713ms/step - accuracy: 0.8803 - loss: 0.2743 - val_accuracy: 0.8245 - val_loss: 0.4003 - val_f1: 0.8177\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.8067\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 664ms/step - accuracy: 0.8965 - loss: 0.2364 - val_accuracy: 0.8140 - val_loss: 0.4287 - val_f1: 0.8067\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.8073\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 688ms/step - accuracy: 0.9236 - loss: 0.1841 - val_accuracy: 0.8115 - val_loss: 0.4679 - val_f1: 0.8073\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 178ms/step\n",
            " - val_f1: 0.8052\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 707ms/step - accuracy: 0.9458 - loss: 0.1359 - val_accuracy: 0.8165 - val_loss: 0.5747 - val_f1: 0.8052\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step\n",
            " - val_f1: 0.8022\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 656ms/step - accuracy: 0.9574 - loss: 0.1042 - val_accuracy: 0.8095 - val_loss: 0.6974 - val_f1: 0.8022\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step - accuracy: 0.8073 - loss: 0.7111\n",
            "Test Accuracy: 0.8095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Bi-LSTM with FastText Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "fTah2KHafIt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2680d26c-9331-4f7d-d47d-b28065b41c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 183ms/step\n",
            " - val_f1: 0.7544\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 611ms/step - accuracy: 0.7471 - loss: 0.4862 - val_accuracy: 0.7720 - val_loss: 0.4271 - val_f1: 0.7544\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 181ms/step\n",
            " - val_f1: 0.7745\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 667ms/step - accuracy: 0.7752 - loss: 0.4164 - val_accuracy: 0.7720 - val_loss: 0.4150 - val_f1: 0.7745\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 186ms/step\n",
            " - val_f1: 0.7767\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 669ms/step - accuracy: 0.7956 - loss: 0.3904 - val_accuracy: 0.7872 - val_loss: 0.4048 - val_f1: 0.7767\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 173ms/step\n",
            " - val_f1: 0.7773\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 685ms/step - accuracy: 0.8163 - loss: 0.3710 - val_accuracy: 0.7945 - val_loss: 0.4036 - val_f1: 0.7773\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 183ms/step\n",
            " - val_f1: 0.7789\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 615ms/step - accuracy: 0.8379 - loss: 0.3372 - val_accuracy: 0.7950 - val_loss: 0.4143 - val_f1: 0.7789\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 183ms/step\n",
            " - val_f1: 0.7865\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 664ms/step - accuracy: 0.8558 - loss: 0.3036 - val_accuracy: 0.7832 - val_loss: 0.4538 - val_f1: 0.7865\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 186ms/step\n",
            " - val_f1: 0.7916\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 623ms/step - accuracy: 0.8875 - loss: 0.2601 - val_accuracy: 0.7943 - val_loss: 0.4587 - val_f1: 0.7916\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 186ms/step\n",
            " - val_f1: 0.7959\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 621ms/step - accuracy: 0.9139 - loss: 0.1995 - val_accuracy: 0.7960 - val_loss: 0.5727 - val_f1: 0.7959\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 184ms/step\n",
            " - val_f1: 0.7816\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 671ms/step - accuracy: 0.9348 - loss: 0.1502 - val_accuracy: 0.7962 - val_loss: 0.6592 - val_f1: 0.7816\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step\n",
            " - val_f1: 0.7914\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 693ms/step - accuracy: 0.9504 - loss: 0.1168 - val_accuracy: 0.7958 - val_loss: 0.7204 - val_f1: 0.7914\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 187ms/step - accuracy: 0.7954 - loss: 0.7109\n",
            "Test Accuracy: 0.7958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training CNN with Word2Vec Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "nRK9k2sEfMRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d48654-65c9-4984-b19c-566c1b23cb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7455\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - accuracy: 0.7287 - loss: 0.5791 - val_accuracy: 0.7897 - val_loss: 0.4091 - val_f1: 0.7455\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8069\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - accuracy: 0.7999 - loss: 0.4054 - val_accuracy: 0.8160 - val_loss: 0.3857 - val_f1: 0.8069\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step\n",
            " - val_f1: 0.8186\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.8152 - loss: 0.3702 - val_accuracy: 0.8195 - val_loss: 0.3854 - val_f1: 0.8186\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8092\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8377 - loss: 0.3393 - val_accuracy: 0.8195 - val_loss: 0.3797 - val_f1: 0.8092\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.8204\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 74ms/step - accuracy: 0.8523 - loss: 0.3076 - val_accuracy: 0.8163 - val_loss: 0.3874 - val_f1: 0.8204\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8122\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 62ms/step - accuracy: 0.8676 - loss: 0.2774 - val_accuracy: 0.8073 - val_loss: 0.4018 - val_f1: 0.8122\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8127\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.8752 - loss: 0.2615 - val_accuracy: 0.8200 - val_loss: 0.4062 - val_f1: 0.8127\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.8171\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9007 - loss: 0.2211 - val_accuracy: 0.8180 - val_loss: 0.4498 - val_f1: 0.8171\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8012\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.9205 - loss: 0.1878 - val_accuracy: 0.8155 - val_loss: 0.5425 - val_f1: 0.8012\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8067\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.9212 - loss: 0.1702 - val_accuracy: 0.8037 - val_loss: 0.4906 - val_f1: 0.8067\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7972 - loss: 0.5177\n",
            "Test Accuracy: 0.8037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training CNN with FastText Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "iVYT0JdefMn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e07309-5544-4f79-841d-72181f0f7a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
            " - val_f1: 0.6427\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - accuracy: 0.7141 - loss: 0.6985 - val_accuracy: 0.7498 - val_loss: 0.4588 - val_f1: 0.6427\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6440\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.7494 - loss: 0.4668 - val_accuracy: 0.7505 - val_loss: 0.4421 - val_f1: 0.6440\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7338\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 61ms/step - accuracy: 0.7609 - loss: 0.4383 - val_accuracy: 0.7695 - val_loss: 0.4333 - val_f1: 0.7338\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.6432\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.7592 - loss: 0.4231 - val_accuracy: 0.7498 - val_loss: 0.4468 - val_f1: 0.6432\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6981\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 64ms/step - accuracy: 0.7562 - loss: 0.4272 - val_accuracy: 0.7638 - val_loss: 0.4328 - val_f1: 0.6981\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6438\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.7671 - loss: 0.4074 - val_accuracy: 0.7500 - val_loss: 0.5331 - val_f1: 0.6438\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
            " - val_f1: 0.7777\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 60ms/step - accuracy: 0.7610 - loss: 0.4048 - val_accuracy: 0.7825 - val_loss: 0.4254 - val_f1: 0.7777\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7855\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - accuracy: 0.7789 - loss: 0.3978 - val_accuracy: 0.7935 - val_loss: 0.4239 - val_f1: 0.7855\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7967\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - accuracy: 0.7862 - loss: 0.3759 - val_accuracy: 0.7915 - val_loss: 0.4152 - val_f1: 0.7967\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7897\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 65ms/step - accuracy: 0.8041 - loss: 0.3688 - val_accuracy: 0.7943 - val_loss: 0.4352 - val_f1: 0.7897\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7869 - loss: 0.4577\n",
            "Test Accuracy: 0.7943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenized"
      ],
      "metadata": {
        "id": "x9YMbGOJinw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "depression_df[\"label\"] = label_encoder.fit_transform(processed_df[\"depression_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "5UKZ21A4iuFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(depression_df[\"tokens\"], depression_df[\"label\"], test_size=0.2, random_state=64)\n",
        "\n",
        "# Train Word2Vec and FastText models\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts.apply(' '.join))  # Join tokens back to text for the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Create embedding matrices\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]\n",
        "\n",
        "max_len = 100  # Max length for padding\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts.apply(' '.join))  # Join tokens for sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts.apply(' '.join))\n",
        "\n",
        "# Padding sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "# Train Bi-LSTM with Word2Vec embeddings\n",
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "H0gEzTcYixt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bba178e-a77a-46c2-a310-c142a8c0a5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step\n",
            " - val_f1: 0.7724\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 314ms/step - accuracy: 0.7491 - loss: 0.4769 - val_accuracy: 0.7653 - val_loss: 0.4281 - val_f1: 0.7724\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step\n",
            " - val_f1: 0.7987\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 313ms/step - accuracy: 0.7780 - loss: 0.4067 - val_accuracy: 0.7987 - val_loss: 0.4005 - val_f1: 0.7987\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step\n",
            " - val_f1: 0.8063\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 304ms/step - accuracy: 0.8057 - loss: 0.3854 - val_accuracy: 0.8169 - val_loss: 0.3807 - val_f1: 0.8063\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step\n",
            " - val_f1: 0.7890\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 305ms/step - accuracy: 0.8305 - loss: 0.3512 - val_accuracy: 0.7775 - val_loss: 0.4114 - val_f1: 0.7890\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.8111\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 312ms/step - accuracy: 0.8422 - loss: 0.3336 - val_accuracy: 0.8062 - val_loss: 0.3901 - val_f1: 0.8111\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 99ms/step\n",
            " - val_f1: 0.8224\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 308ms/step - accuracy: 0.8590 - loss: 0.3059 - val_accuracy: 0.8206 - val_loss: 0.3853 - val_f1: 0.8224\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 91ms/step\n",
            " - val_f1: 0.8113\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 306ms/step - accuracy: 0.8904 - loss: 0.2512 - val_accuracy: 0.8134 - val_loss: 0.4284 - val_f1: 0.8113\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.7983\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 306ms/step - accuracy: 0.9117 - loss: 0.2045 - val_accuracy: 0.7919 - val_loss: 0.4999 - val_f1: 0.7983\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step\n",
            " - val_f1: 0.8088\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 305ms/step - accuracy: 0.9359 - loss: 0.1585 - val_accuracy: 0.8094 - val_loss: 0.5232 - val_f1: 0.8088\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step\n",
            " - val_f1: 0.7997\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 335ms/step - accuracy: 0.9588 - loss: 0.1091 - val_accuracy: 0.8034 - val_loss: 0.7043 - val_f1: 0.7997\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 85ms/step - accuracy: 0.7844 - loss: 0.7463\n",
            "Test Accuracy: 0.8034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teenager / Non-Teenager\n"
      ],
      "metadata": {
        "id": "BowN0hrtgd84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teenager_df['teenager_class'] = teenager_df['class'].apply(lambda x: 'teenager' if x == 'teenagers' else 'nonteenager')"
      ],
      "metadata": {
        "id": "5JBG2bf-gjLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "teenager_df[\"label\"] = label_encoder.fit_transform(teenager_df[\"teenager_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "tb7dCmqJg3sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization/Embedding"
      ],
      "metadata": {
        "id": "L7opjK5IjMnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding sequences to have the same length\n",
        "max_len = 200  # Max length for padding\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "RLObioXzjP2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]"
      ],
      "metadata": {
        "id": "dN11WPKMjkgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "kx0b_T0XjmV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68156686-43b0-4d53-ba1f-1cba8b288481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step\n",
            " - val_f1: 0.7761\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 604ms/step - accuracy: 0.7602 - loss: 0.4726 - val_accuracy: 0.7922 - val_loss: 0.4083 - val_f1: 0.7761\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step\n",
            " - val_f1: 0.7686\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 595ms/step - accuracy: 0.7960 - loss: 0.4024 - val_accuracy: 0.7891 - val_loss: 0.3990 - val_f1: 0.7686\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step\n",
            " - val_f1: 0.7953\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 592ms/step - accuracy: 0.8080 - loss: 0.3873 - val_accuracy: 0.8034 - val_loss: 0.3856 - val_f1: 0.7953\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step\n",
            " - val_f1: 0.7965\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 591ms/step - accuracy: 0.8294 - loss: 0.3528 - val_accuracy: 0.8037 - val_loss: 0.3925 - val_f1: 0.7965\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 238ms/step\n",
            " - val_f1: 0.8147\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 601ms/step - accuracy: 0.8301 - loss: 0.3487 - val_accuracy: 0.8228 - val_loss: 0.3884 - val_f1: 0.8147\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 178ms/step\n",
            " - val_f1: 0.8009\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 624ms/step - accuracy: 0.8601 - loss: 0.3047 - val_accuracy: 0.8041 - val_loss: 0.4018 - val_f1: 0.8009\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 175ms/step\n",
            " - val_f1: 0.8150\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 600ms/step - accuracy: 0.8761 - loss: 0.2706 - val_accuracy: 0.8119 - val_loss: 0.4087 - val_f1: 0.8150\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step\n",
            " - val_f1: 0.8095\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 595ms/step - accuracy: 0.9081 - loss: 0.2170 - val_accuracy: 0.8122 - val_loss: 0.4500 - val_f1: 0.8095\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step\n",
            " - val_f1: 0.8119\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 592ms/step - accuracy: 0.9277 - loss: 0.1655 - val_accuracy: 0.8128 - val_loss: 0.5284 - val_f1: 0.8119\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 178ms/step\n",
            " - val_f1: 0.8089\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 662ms/step - accuracy: 0.9486 - loss: 0.1255 - val_accuracy: 0.8128 - val_loss: 0.6010 - val_f1: 0.8089\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.8024 - loss: 0.6159\n",
            "Test Accuracy: 0.8128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Bi-LSTM with FastText Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "RDGUu-XJfDiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acdf727-77b3-44aa-ec8e-e153aba52e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step\n",
            " - val_f1: 0.7654\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 596ms/step - accuracy: 0.7525 - loss: 0.4873 - val_accuracy: 0.7722 - val_loss: 0.4323 - val_f1: 0.7654\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step\n",
            " - val_f1: 0.7682\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 605ms/step - accuracy: 0.7839 - loss: 0.4079 - val_accuracy: 0.7750 - val_loss: 0.4535 - val_f1: 0.7682\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step\n",
            " - val_f1: 0.7843\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 646ms/step - accuracy: 0.7914 - loss: 0.4023 - val_accuracy: 0.8016 - val_loss: 0.4325 - val_f1: 0.7843\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step\n",
            " - val_f1: 0.7626\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 604ms/step - accuracy: 0.8218 - loss: 0.3600 - val_accuracy: 0.7866 - val_loss: 0.4055 - val_f1: 0.7626\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 173ms/step\n",
            " - val_f1: 0.8025\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 599ms/step - accuracy: 0.8514 - loss: 0.3278 - val_accuracy: 0.8050 - val_loss: 0.3976 - val_f1: 0.8025\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 175ms/step\n",
            " - val_f1: 0.8060\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 650ms/step - accuracy: 0.8715 - loss: 0.2865 - val_accuracy: 0.8125 - val_loss: 0.4192 - val_f1: 0.8060\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step\n",
            " - val_f1: 0.8003\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 597ms/step - accuracy: 0.8947 - loss: 0.2433 - val_accuracy: 0.8066 - val_loss: 0.4425 - val_f1: 0.8003\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 178ms/step\n",
            " - val_f1: 0.7967\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 596ms/step - accuracy: 0.9204 - loss: 0.1912 - val_accuracy: 0.8034 - val_loss: 0.5077 - val_f1: 0.7967\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step\n",
            " - val_f1: 0.7964\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 615ms/step - accuracy: 0.9377 - loss: 0.1524 - val_accuracy: 0.8041 - val_loss: 0.5994 - val_f1: 0.7964\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step\n",
            " - val_f1: 0.7923\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 615ms/step - accuracy: 0.9577 - loss: 0.1070 - val_accuracy: 0.7953 - val_loss: 0.6952 - val_f1: 0.7923\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.7816 - loss: 0.7297\n",
            "Test Accuracy: 0.7953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training CNN with Word2Vec Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5KydmbcelRi",
        "outputId": "f26b2f3f-530c-4249-d8c9-d4aad16b74e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7940\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - accuracy: 0.7352 - loss: 0.5898 - val_accuracy: 0.7912 - val_loss: 0.4059 - val_f1: 0.7940\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
            " - val_f1: 0.8070\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.8074 - loss: 0.3904 - val_accuracy: 0.8103 - val_loss: 0.3935 - val_f1: 0.8070\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8138\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 58ms/step - accuracy: 0.8265 - loss: 0.3597 - val_accuracy: 0.8234 - val_loss: 0.3824 - val_f1: 0.8138\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8082\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - accuracy: 0.8378 - loss: 0.3327 - val_accuracy: 0.8181 - val_loss: 0.3865 - val_f1: 0.8082\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8040\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.8466 - loss: 0.3129 - val_accuracy: 0.8194 - val_loss: 0.4016 - val_f1: 0.8040\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8068\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.8753 - loss: 0.2626 - val_accuracy: 0.8034 - val_loss: 0.4078 - val_f1: 0.8068\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
            " - val_f1: 0.8097\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 56ms/step - accuracy: 0.8991 - loss: 0.2279 - val_accuracy: 0.8084 - val_loss: 0.4662 - val_f1: 0.8097\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8041\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.9171 - loss: 0.1923 - val_accuracy: 0.7978 - val_loss: 0.5028 - val_f1: 0.8041\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8110\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 59ms/step - accuracy: 0.9386 - loss: 0.1489 - val_accuracy: 0.8166 - val_loss: 0.4486 - val_f1: 0.8110\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.8044\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.9475 - loss: 0.1299 - val_accuracy: 0.8116 - val_loss: 0.6167 - val_f1: 0.8044\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8051 - loss: 0.6441\n",
            "Test Accuracy: 0.8116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training CNN with FastText Embeddings...\")\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_matrix_ft, num_classes)\n",
        "train_and_evaluate(cnn_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "j2biIiiweqaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965b965a-2f97-4519-e17f-bdfdf4d3b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN with FastText Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6408\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - accuracy: 0.7052 - loss: 0.8155 - val_accuracy: 0.7484 - val_loss: 0.4657 - val_f1: 0.6408\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
            " - val_f1: 0.6408\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.7511 - loss: 0.4715 - val_accuracy: 0.7484 - val_loss: 0.4603 - val_f1: 0.6408\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7165\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 60ms/step - accuracy: 0.7545 - loss: 0.4477 - val_accuracy: 0.7600 - val_loss: 0.4434 - val_f1: 0.7165\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.7633\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.7592 - loss: 0.4374 - val_accuracy: 0.7641 - val_loss: 0.4333 - val_f1: 0.7633\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n",
            " - val_f1: 0.7352\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 57ms/step - accuracy: 0.7681 - loss: 0.4187 - val_accuracy: 0.7653 - val_loss: 0.4312 - val_f1: 0.7352\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.7773\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 64ms/step - accuracy: 0.7734 - loss: 0.4148 - val_accuracy: 0.7812 - val_loss: 0.4242 - val_f1: 0.7773\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.7714\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.7799 - loss: 0.3986 - val_accuracy: 0.7622 - val_loss: 0.4301 - val_f1: 0.7714\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step\n",
            " - val_f1: 0.7727\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.7694 - loss: 0.3895 - val_accuracy: 0.7794 - val_loss: 0.4193 - val_f1: 0.7727\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            " - val_f1: 0.6535\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: 0.7912 - loss: 0.3701 - val_accuracy: 0.7534 - val_loss: 0.4291 - val_f1: 0.6535\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            " - val_f1: 0.7746\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.8012 - loss: 0.3580 - val_accuracy: 0.7800 - val_loss: 0.4200 - val_f1: 0.7746\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7611 - loss: 0.4472\n",
            "Test Accuracy: 0.7800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training RNN with Word2Vec Embeddings...\")\n",
        "rnn_model = build_rnn_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(rnn_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt5pGy19s220",
        "outputId": "677ea745-15d0-4b9f-a404-b81a2a095f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 107ms/step\n",
            " - val_f1: 0.6494\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 339ms/step - accuracy: 0.7350 - loss: 0.5581 - val_accuracy: 0.7509 - val_loss: 0.5191 - val_f1: 0.6494\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step\n",
            " - val_f1: 0.7195\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 339ms/step - accuracy: 0.7595 - loss: 0.5219 - val_accuracy: 0.7525 - val_loss: 0.4787 - val_f1: 0.7195\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step\n",
            " - val_f1: 0.7231\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 338ms/step - accuracy: 0.7638 - loss: 0.4963 - val_accuracy: 0.7681 - val_loss: 0.4679 - val_f1: 0.7231\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step\n",
            " - val_f1: 0.8087\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 336ms/step - accuracy: 0.7888 - loss: 0.4311 - val_accuracy: 0.8087 - val_loss: 0.3945 - val_f1: 0.8087\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step\n",
            " - val_f1: 0.8129\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 364ms/step - accuracy: 0.8177 - loss: 0.3733 - val_accuracy: 0.8081 - val_loss: 0.3833 - val_f1: 0.8129\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step\n",
            " - val_f1: 0.8239\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 338ms/step - accuracy: 0.8346 - loss: 0.3581 - val_accuracy: 0.8269 - val_loss: 0.3726 - val_f1: 0.8239\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 109ms/step\n",
            " - val_f1: 0.8212\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 340ms/step - accuracy: 0.8475 - loss: 0.3372 - val_accuracy: 0.8194 - val_loss: 0.3864 - val_f1: 0.8212\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 112ms/step\n",
            " - val_f1: 0.8196\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 340ms/step - accuracy: 0.8671 - loss: 0.3062 - val_accuracy: 0.8303 - val_loss: 0.3764 - val_f1: 0.8196\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step\n",
            " - val_f1: 0.8215\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 363ms/step - accuracy: 0.8679 - loss: 0.2974 - val_accuracy: 0.8328 - val_loss: 0.3861 - val_f1: 0.8215\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 109ms/step\n",
            " - val_f1: 0.8283\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 340ms/step - accuracy: 0.8896 - loss: 0.2702 - val_accuracy: 0.8278 - val_loss: 0.4200 - val_f1: 0.8283\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 107ms/step - accuracy: 0.8152 - loss: 0.4257\n",
            "Test Accuracy: 0.8278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenized"
      ],
      "metadata": {
        "id": "jG0Ks3MLjn-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "processed_df[\"label\"] = label_encoder.fit_transform(processed_df[\"teenager_class\"])\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "rDc3vpCejswN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(depression_df[\"tokens\"], depression_df[\"label\"], test_size=0.2, random_state=64)\n",
        "\n",
        "# Train Word2Vec and FastText models\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "fasttext_model = gensim.models.FastText(sentences=train_texts.tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts.apply(' '.join))  # Join tokens back to text for the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Create embedding matrices\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, 100))\n",
        "embedding_matrix_ft = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix_w2v[i] = word2vec_model.wv[word]\n",
        "    if word in fasttext_model.wv:\n",
        "        embedding_matrix_ft[i] = fasttext_model.wv[word]\n",
        "\n",
        "max_len = 100  # Max length for padding\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts.apply(' '.join))  # Join tokens for sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts.apply(' '.join))\n",
        "\n",
        "# Padding sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "# Train Bi-LSTM with Word2Vec embeddings\n",
        "print(\"Training Bi-LSTM with Word2Vec Embeddings...\")\n",
        "bilstm_model = build_bilstm_model(vocab_size, embedding_matrix_w2v, num_classes)\n",
        "train_and_evaluate(bilstm_model, train_padded, train_labels, test_padded, test_labels)"
      ],
      "metadata": {
        "id": "tFxv_y6ajtND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036a1246-0b07-4105-e248-367e87d7b9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bi-LSTM with Word2Vec Embeddings...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step\n",
            " - val_f1: 0.7885\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 309ms/step - accuracy: 0.7518 - loss: 0.4708 - val_accuracy: 0.7841 - val_loss: 0.4099 - val_f1: 0.7885\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.7844\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 303ms/step - accuracy: 0.7957 - loss: 0.3935 - val_accuracy: 0.7741 - val_loss: 0.4236 - val_f1: 0.7844\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step\n",
            " - val_f1: 0.7760\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 305ms/step - accuracy: 0.8237 - loss: 0.3580 - val_accuracy: 0.8019 - val_loss: 0.4104 - val_f1: 0.7760\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.8206\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 308ms/step - accuracy: 0.8336 - loss: 0.3517 - val_accuracy: 0.8231 - val_loss: 0.3861 - val_f1: 0.8206\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.8075\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 302ms/step - accuracy: 0.8519 - loss: 0.3103 - val_accuracy: 0.8216 - val_loss: 0.4222 - val_f1: 0.8075\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.8119\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 302ms/step - accuracy: 0.8783 - loss: 0.2707 - val_accuracy: 0.8159 - val_loss: 0.4149 - val_f1: 0.8119\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step\n",
            " - val_f1: 0.8090\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 303ms/step - accuracy: 0.9037 - loss: 0.2196 - val_accuracy: 0.8197 - val_loss: 0.4987 - val_f1: 0.8090\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step\n",
            " - val_f1: 0.7952\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 303ms/step - accuracy: 0.9312 - loss: 0.1642 - val_accuracy: 0.8022 - val_loss: 0.5344 - val_f1: 0.7952\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step\n",
            " - val_f1: 0.8064\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 302ms/step - accuracy: 0.9537 - loss: 0.1185 - val_accuracy: 0.8062 - val_loss: 0.6301 - val_f1: 0.8064\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step\n",
            " - val_f1: 0.8007\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 305ms/step - accuracy: 0.9662 - loss: 0.0905 - val_accuracy: 0.8003 - val_loss: 0.6760 - val_f1: 0.8007\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.7857 - loss: 0.7049\n",
            "Test Accuracy: 0.8003\n"
          ]
        }
      ]
    }
  ]
}